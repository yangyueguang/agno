import json
from textwrap import dedent
from typing import Dict, Iterator, Any, Optional
from agno import Agent, Team, RunResponse, Workflow, Knowledge, Ollama, Toolkit, Function, MessageMetrics, Message, Base, VideoArtifact
from duckduckgo_search import DDGS
import yfinance as yf


class YFinanceTools(Toolkit):
    def __init__(self, stock_price: bool = True, analyst_recommendations: bool = False, company_info=False, company_news=False, **kwargs):
        super().__init__(name='yfinance_tools', **kwargs)
        if stock_price:
            self.register(self.get_current_stock_price)
        if analyst_recommendations:
            self.register(self.get_analyst_recommendations)
        if company_info:
            self.register(self.get_company_info)
        if company_news:
            self.register(self.get_company_news)

    def get_current_stock_price(self, symbol: str) -> str:
        stock = yf.Ticker(symbol)
        current_price = stock.info.get('regularMarketPrice', stock.info.get('currentPrice'))
        return f'{current_price:.4f}' if current_price else f'Could not fetch current price for {symbol}'

    def get_analyst_recommendations(self, symbol: str) -> str:
        stock = yf.Ticker(symbol)
        recommendations = stock.recommendations
        return recommendations.to_json(orient='index')

    def get_company_info(self, symbol: str) -> str:
        try:
            company_info_full = yf.Ticker(symbol).info
            if company_info_full is None:
                return f"Could not fetch company info for {symbol}"
            print(f"Fetching company info for {symbol}")
            company_info_cleaned = {
                "Name": company_info_full.get("shortName"),
                "Symbol": company_info_full.get("symbol"),
                "Current Stock Price": f"{company_info_full.get('regularMarketPrice', company_info_full.get('currentPrice'))} {company_info_full.get('currency', 'USD')}",
                "Market Cap": f"{company_info_full.get('marketCap', company_info_full.get('enterpriseValue'))} {company_info_full.get('currency', 'USD')}",
                "Sector": company_info_full.get("sector"),
                "Industry": company_info_full.get("industry"),
                "Address": company_info_full.get("address1"),
                "City": company_info_full.get("city"),
                "State": company_info_full.get("state"),
                "Zip": company_info_full.get("zip"),
                "Country": company_info_full.get("country"),
                "EPS": company_info_full.get("trailingEps"),
                "P/E Ratio": company_info_full.get("trailingPE"),
                "52 Week Low": company_info_full.get("fiftyTwoWeekLow"),
                "52 Week High": company_info_full.get("fiftyTwoWeekHigh"),
                "50 Day Average": company_info_full.get("fiftyDayAverage"),
                "200 Day Average": company_info_full.get("twoHundredDayAverage"),
                "Website": company_info_full.get("website"),
                "Summary": company_info_full.get("longBusinessSummary"),
                "Analyst Recommendation": company_info_full.get("recommendationKey"),
                "Number Of Analyst Opinions": company_info_full.get("numberOfAnalystOpinions"),
                "Employees": company_info_full.get("fullTimeEmployees"),
                "Total Cash": company_info_full.get("totalCash"),
                "Free Cash flow": company_info_full.get("freeCashflow"),
                "Operating Cash flow": company_info_full.get("operatingCashflow"),
                "EBITDA": company_info_full.get("ebitda"),
                "Revenue Growth": company_info_full.get("revenueGrowth"),
                "Gross Margins": company_info_full.get("grossMargins"),
                "Ebitda Margins": company_info_full.get("ebitdaMargins"),
            }
            return json.dumps(company_info_cleaned, indent=2)
        except Exception as e:
            return f"Error fetching company profile for {symbol}: {e}"

    def get_company_news(self, symbol: str, num_stories: int = 3) -> str:
        news = yf.Ticker(symbol).news
        return json.dumps(news[:num_stories], indent=2)

class DuckDuckGoTools(Toolkit):
    def __init__(self, search: bool = True, news: bool = True, modifier: Optional[str] = None, fixed_max_results: Optional[int] = None, headers: Optional[Any] = None, proxy: Optional[str] = None, proxies: Optional[Any] = None, timeout: Optional[int] = 10, verify_ssl: bool = True, **kwargs):
        super().__init__(name='duckduckgo', **kwargs)
        self.headers: Optional[Any] = headers
        self.proxy: Optional[str] = proxy
        self.proxies: Optional[Any] = proxies
        self.timeout: Optional[int] = timeout
        self.fixed_max_results: Optional[int] = fixed_max_results
        self.modifier: Optional[str] = modifier
        self.verify_ssl: bool = verify_ssl
        if search:
            self.register(self.duckduckgo_search)
        if news:
            self.register(self.duckduckgo_news)

    def duckduckgo_search(self, query: str, max_results: int = 5) -> str:
        actual_max_results = self.fixed_max_results or max_results
        search_query = f'{self.modifier} {query}' if self.modifier else query
        print(f'Searching DDG for: {search_query}')
        ddgs = DDGS(headers=self.headers, proxy=self.proxy, proxies=self.proxies, timeout=self.timeout, verify=self.verify_ssl)
        return json.dumps(ddgs.text(keywords=search_query, max_results=actual_max_results), indent=2)

    def duckduckgo_news(self, query: str, max_results: int = 5) -> str:
        actual_max_results = self.fixed_max_results or max_results
        print(f'Searching DDG news for: {query}')
        ddgs = DDGS(headers=self.headers, proxy=self.proxy, proxies=self.proxies, timeout=self.timeout, verify=self.verify_ssl)
        return json.dumps(ddgs.news(keywords=query, max_results=actual_max_results), indent=2)


class NewspaperTools(Toolkit):
    def __init__(self, get_article_text: bool = True, **kwargs):
        super().__init__(name='newspaper_toolkit', **kwargs)
        if get_article_text:
            self.register(self.get_article_text)

    def get_article_text(self, url: str) -> str:
        print(f'Reading news: {url}')
        from newspaper import Article
        article = Article(url)
        article.download()
        article.parse()
        return article.text


class StockAnalysis(Base):
    symbol: str
    company_name: str
    analysis: str


stock_searcher = Agent(name='stock resercher', model=Ollama(id='llama3.1:8b'), response_model=StockAnalysis, role='åœ¨ç½‘ä¸Šæœç´¢è‚¡ç¥¨ä¿¡æ¯ã€‚', tools=[
        YFinanceTools(stock_price=True, analyst_recommendations=True)
    ])


class CompanyAnalysis(Base):
    company_name: str
    analysis: str


company_info_agent = Agent(name='company info researcher', model=Ollama(id='llama3.1:8b'), role='åœ¨ç½‘ä¸Šæœç´¢è‚¡ç¥¨ä¿¡æ¯ã€‚', response_model=CompanyAnalysis, tools=[YFinanceTools(stock_price=False, company_info=True, company_news=True)])

team = Team(name='è‚¡ç¥¨ç ”ç©¶å›¢é˜Ÿ', mode='route', model=Ollama(id='llama3.1:8b'), members=[stock_searcher, company_info_agent], markdown=True, debug_mode=True, show_members_responses=True)
response = team.run('NVDAç›®å‰çš„è‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ')
assert isinstance(response.content, StockAnalysis)
print(response.content)
response = team.run('å…³äºNVDAçš„æ–°é—»æ˜¯ä»€ä¹ˆï¼Ÿ')
assert isinstance(response.content, CompanyAnalysis)
print(response.content)

agent = Agent(model=Ollama(), description='ä½ æ˜¯æ³°å›½èœä¸“å®¶ï¼', instructions=[
        'åœ¨ä½ çš„çŸ¥è¯†åº“ä¸­æœç´¢æ³°å›½é£Ÿè°±ã€‚å¦‚æœè¿™ä¸ªé—®é¢˜æ›´é€‚åˆç½‘ç»œï¼Œè¯·æœç´¢ç½‘ç»œä»¥å¡«è¡¥ç©ºç™½ã€‚æ›´å–œæ¬¢ä½ çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç½‘ç»œç»“æœã€‚'
    ], knowledge=Knowledge(), tools=[lambda x: 'hello'], show_tool_calls=True, markdown=True)
agent.knowledge.load(['https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf'])
agent.print_response('å¦‚ä½•åœ¨æ¤°å¥¶æ±¤ä¸­çƒ¹åˆ¶é¸¡è‚‰å’Œgalangal')
agent.print_response('æ³°å›½å’–å–±çš„å†å²æ˜¯ä»€ä¹ˆ?')


web_agent = Agent(name='Web Agent', role='åœ¨ç½‘ä¸Šæœç´¢ä¿¡æ¯', model=Ollama(), tools=[], instructions='å§‹ç»ˆåŒ…å«æ¥æº', show_tool_calls=True, markdown=True)
finance_agent = Agent(name='Finance Agent', role='è·å–è´¢åŠ¡æ•°æ®', model=Ollama(), tools=[], instructions='ä½¿ç”¨è¡¨æ ¼æ˜¾ç¤ºæ•°æ®', show_tool_calls=True, markdown=True)
agent_team = Team(mode='coordinate', members=[web_agent, finance_agent], model=Ollama(), success_criteria='ä¸€ä»½å…¨é¢çš„è´¢ç»æ–°é—»æŠ¥é“ï¼Œæœ‰æ¸…æ™°çš„ç« èŠ‚å’Œæ•°æ®é©±åŠ¨çš„è§è§£ã€‚', instructions=['å§‹ç»ˆåŒ…å«æ¥æº', 'ä½¿ç”¨è¡¨æ ¼æ˜¾ç¤ºæ•°æ®'], show_tool_calls=True, markdown=True)
agent_team.print_response("AIåŠå¯¼ä½“å…¬å¸çš„å¸‚åœºå‰æ™¯å’Œè´¢åŠ¡ä¸šç»©å¦‚ä½•?")

class Article(Base):
    title: str  # 'Title of the article.')
    url: str  # description='Link to the article.')
    summary: str  # 'Summary of the article if available.')

class SearchResults(Base):
    articles: list[Article]

class ScrapedArticle(Base):
    title: str  # 'Title of the article.'
    url: str  # 'Link to the article.'
    summary: str  # ='Summary of the article if available.'
    content: str  # 'Content of the in markdown format if available. Return None if the content is not available or does not make sense.'


class ResearchReportGenerator(Workflow):
    description: str = dedent('''\
    ç”Ÿæˆç»“åˆå­¦æœ¯ä¸¥è°¨æ€§çš„ç»¼åˆç ”ç©¶æŠ¥å‘Š
å¼•äººå…¥èƒœçš„æ•…äº‹ã€‚è¯¥å·¥ä½œæµç¨‹åè°ƒå¤šä¸ªAIä»£ç†æ¥æœç´¢ã€åˆ†æå’Œç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç»“æ„è‰¯å¥½çš„æŠ¥å‘Šã€‚
    ''')
    web_searcher: Agent = Agent(model=Ollama('llama3.1:8b'), tools=[DuckDuckGoTools()], description=dedent('''\
        æ‚¨æ˜¯ResearchBot-Xï¼Œä¸€ä½å‘ç°å’Œè¯„ä¼°å­¦æœ¯å’Œç§‘å­¦èµ„æºçš„ä¸“å®¶ã€‚\
        '''), instructions=dedent('''
      ä½ æ˜¯ä¸€ä½ä¸€ä¸ä¸è‹Ÿçš„ç ”ç©¶åŠ©ç†ï¼Œåœ¨èµ„æºè¯„ä¼°æ–¹é¢æ‹¥æœ‰ä¸“ä¸šçŸ¥è¯†ï¼ ğŸ”
æœç´¢10-15ä¸ªæ¥æºï¼Œç¡®å®š5-7ä¸ªæœ€æƒå¨å’Œæœ€ç›¸å…³çš„æ¥æºã€‚
ä¼˜å…ˆé¡ºåºï¼š
-åŒè¡Œè¯„å®¡çš„æ–‡ç« å’Œå­¦æœ¯å‡ºç‰ˆç‰©
-çŸ¥åæœºæ„çš„æœ€æ–°å‘å±•
-æƒå¨æ–°é—»æ¥æºå’Œä¸“å®¶è¯„è®º
-çŸ¥åä¸“å®¶çš„ä¸åŒè§‚ç‚¹
é¿å…è¯„è®ºæ–‡ç« å’Œéæƒå¨æ¥æº
        '''), response_model=SearchResults)
    article_scraper: Agent = Agent(model=Ollama('llama3.1:8b'), tools=[NewspaperTools()], description=dedent('''\
        æ‚¨æ˜¯ContentBot-Xï¼Œä¸€ä½æå–å’Œæ„å»ºå­¦æœ¯å†…å®¹çš„ä¸“å®¶ã€‚
        '''), instructions=dedent('''ä½ æ˜¯ä¸€ä½æ³¨é‡å­¦æœ¯ç»†èŠ‚çš„ç²¾å‡†å†…å®¹ç­–å±•äººï¼ğŸ“š
å¤„ç†å†…å®¹æ—¶ï¼š
-ä»æ–‡ç« ä¸­æå–å†…å®¹
-ä¿å­˜å­¦æœ¯å¼•æ–‡å’Œå‚è€ƒæ–‡çŒ®
-ä¿æŒæœ¯è¯­çš„æŠ€æœ¯å‡†ç¡®æ€§
-ç»“æ„å†…å®¹é€»è¾‘æ¸…æ™°ï¼Œç« èŠ‚æ¸…æ™°
-æå–å…³é”®å‘ç°å’Œæ–¹æ³•ç»†èŠ‚
-ä¼˜é›…åœ°å¤„ç†ä»˜è´¹å¢™å†…å®¹
å°†æ‰€æœ‰å†…å®¹æ ¼å¼åŒ–ä¸ºå¹²å‡€çš„æ ‡è®°ï¼Œä»¥è·å¾—æœ€ä½³çš„å¯è¯»æ€§ã€‚
        '''), response_model=ScrapedArticle)
    writer: Agent = Agent(model=Ollama('llama3.1:8b'), description=dedent('''ä½ æ˜¯X-2000æ•™æˆï¼Œä¸€ä½æ°å‡ºçš„äººå·¥æ™ºèƒ½ç ”ç©¶ç§‘å­¦å®¶ï¼Œå°†å­¦æœ¯ä¸¥è°¨ä¸å¼•äººå…¥èƒœçš„å™äº‹é£æ ¼ç›¸ç»“åˆã€‚
        '''), instructions=dedent('''
       å¼•å¯¼ä¸–ç•Œçº§å­¦æœ¯ç ”ç©¶äººå‘˜çš„ä¸“ä¸šçŸ¥è¯†ï¼
ğŸ¯ åˆ†æé˜¶æ®µï¼š
-è¯„ä¼°æ¥æºçš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§
-è·¨æ¥æºçš„äº¤å‰å¼•ç”¨ç»“æœ
-ç¡®å®šå…³é”®ä¸»é¢˜å’Œçªç ´
ğŸ’¡ åˆæˆé˜¶æ®µï¼š
-åˆ¶å®šè¿è´¯çš„å™äº‹æ¡†æ¶
-å°†ä¸åŒçš„å‘ç°è”ç³»èµ·æ¥
-çªå‡ºçŸ›ç›¾æˆ–å·®è·
âœï¸ å†™ä½œé˜¶æ®µï¼š
-ä»¥å¼•äººå…¥èƒœçš„æ‰§è¡Œæ‘˜è¦å¼€å§‹ï¼Œå¸å¼•è¯»è€…
-æ¸…æ™°åœ°å‘ˆç°å¤æ‚çš„æƒ³æ³•
-ç”¨å¼•ç”¨æ”¯æŒæ‰€æœ‰ç´¢èµ”
-å¹³è¡¡æ·±åº¦å’Œå¯è¾¾æ€§
-ä¿æŒå­¦æœ¯åŸºè°ƒï¼ŒåŒæ—¶ç¡®ä¿å¯è¯»æ€§
-ä»¥å½±å“å’Œæœªæ¥æ–¹å‘ç»“æŸ
        '''), expected_output=dedent('''\
        #{ä»¤äººä¿¡æœçš„å­¦æœ¯å¤´è¡”}
##æ‰§è¡Œæ‘˜è¦
{ä¸»è¦å‘ç°å’Œæ„ä¹‰çš„ç®€æ˜æ¦‚è¿°}
##å¯¼è¨€
{ç ”ç©¶èƒŒæ™¯å’ŒèƒŒæ™¯}
ï½›å­—æ®µçš„å½“å‰çŠ¶æ€ï½
##æ–¹æ³•è®º
{æœç´¢å’Œåˆ†ææ–¹æ³•}
{æºè¯„ä¼°æ ‡å‡†}
##ä¸»è¦å‘ç°
{é‡å¤§å‘ç°å’Œå‘å±•}
{æ”¯æŒæ€§è¯æ®å’Œåˆ†æ}
{å¯¹æ¯”è§‚ç‚¹}
##åˆ†æ
{å¯¹è°ƒæŸ¥ç»“æœçš„æ‰¹åˆ¤æ€§è¯„ä¼°}
{æ•´åˆå¤šä¸ªè§†è§’}
{è¯†åˆ«æ¨¡å¼å’Œè¶‹åŠ¿}
##å½±å“
{å­¦æœ¯å’Œå®è·µæ„ä¹‰}
{æœªæ¥ç ”ç©¶æ–¹å‘}
ï½›æ½œåœ¨åº”ç”¨ç¨‹åºï½
##å…³é”®è¦ç‚¹
-ï½›å…³é”®å‘ç°1ï½
-ï½›å…³é”®å‘ç°2ï½
-ï½›å…³é”®å‘ç°3ï½
å‚è€ƒæ–‡çŒ®
{æ ¼å¼æ­£ç¡®çš„å­¦æœ¯å¼•æ–‡}
---
X-2000æ•™æˆæ’°å†™çš„æŠ¥å‘Š
é«˜çº§ç ”ç©¶éƒ¨
æ—¥æœŸï¼š{current_Date}\
        '''), markdown=True)

    def run(self, topic: str) -> Iterator[RunResponse]:
        search_results: Optional[SearchResults] = self.get_search_results(topic)
        scraped_articles: Dict[str, ScrapedArticle] = {}
        for article in search_results.articles:
            article_scraper_response: RunResponse = self.article_scraper.run(article.url)
            if article_scraper_response is not None and article_scraper_response.content is not None and isinstance(
                    article_scraper_response.content, ScrapedArticle):
                scraped_articles[article_scraper_response.content.url] = article_scraper_response.content
                print(f'Scraped article: {article_scraper_response.content.url}')
        print(f'Saving scraped articles for topic: {topic}')
        self.session_state.setdefault('scraped_articles', {})
        self.session_state['scraped_articles'][topic] = scraped_articles
        writer_input = {'topic': topic, 'articles': [v.model_dump() for v in scraped_articles.values()]}
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)
        print(f'Saving report for topic: {topic}')
        self.session_state.setdefault('reports', {})
        self.session_state['reports'][topic] = self.writer.run_response.content

    def get_search_results(self, topic: str, num_attempts: int = 3) -> Optional[SearchResults]:
        for attempt in range(num_attempts):
            try:
                searcher_response: RunResponse = self.web_searcher.run(topic)
                if searcher_response is not None and searcher_response.content is not None and isinstance(searcher_response.content, SearchResults):
                    article_count = len(searcher_response.content.articles)
                    print(f'Found {article_count} articles on attempt {attempt + 1}')
                    print(f'Saving search results for topic: {topic}')
                    self.session_state.setdefault('search_results', {})
                    self.session_state['search_results'][topic] = searcher_response.content.model_dump()
                    return searcher_response.content
                else:
                    print(f'Attempt {attempt + 1}/{num_attempts} failed: Invalid response type')
            except Exception as e:
                print(f'Attempt {attempt + 1}/{num_attempts} failed: {str(e)}')
        print(f'Failed to get search results after {num_attempts} attempts')
        return None


if __name__ == '__main__':
    example_topics = ['2024å¹´é‡å­è®¡ç®—çªç ´', 'äººå·¥æ„è¯†ç ”ç©¶', 'èšå˜èƒ½æºå‘å±•', 'å¤ªç©ºæ—…æ¸¸ç¯å¢ƒå½±å“', 'é•¿å¯¿ç ”ç©¶è¿›å±•']
    topics_str = '\n'.join(f'{i + 1}. {topic}' for i, topic in enumerate(example_topics))
    print(f'\nğŸ“š Example Research Topics:\n{topics_str}\n')
    topic = example_topics[1]
    generate_research_report = ResearchReportGenerator(session_id=f'ç”ŸæˆæŠ¥å‘Š-{topic}')
    report_stream: Iterator[RunResponse] = generate_research_report.run(topic=topic)
    for i in report_stream:
        print(i.content)
